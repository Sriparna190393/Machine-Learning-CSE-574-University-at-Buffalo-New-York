{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 569)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5bfdb9584d07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m#Calculating confusion matrix metrics for training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mTPtr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTNtr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFPtr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFNtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0maccuracy_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTPtr\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mTNtr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTPtr\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mTNtr\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mFPtr\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mFNtr\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m#accuracy for training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-5bfdb9584d07>\u001b[0m in \u001b[0;36mpredict_data\u001b[0;34m(X_data, Y_data, W, bias)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpred_Y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mY_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpred_Y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mTP\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mpred_Y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mY_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpred_Y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mTN\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpred_Y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mY_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpred_Y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler,Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#DATA PREPROCESSING\n",
    "\n",
    "#read data from file\n",
    "data=pd.read_csv('wdbc.csv',header = None)\n",
    "\n",
    "#Map values of B/M to 0/1\n",
    "data[1] = data[1].apply({'B':0, 'M':1}.get)\n",
    "\n",
    "#drop the first column : ID\n",
    "to_drop1=[0]\n",
    "data.drop(to_drop1,inplace=True,axis=1)\n",
    "\n",
    "#splitting the data into training, validation and testing sets\n",
    "up_data=data\n",
    "\n",
    "Y=np.array([data[1]])   #Output data space\n",
    "print(Y.shape)\n",
    "Y=Y.transpose()\n",
    "\n",
    "to_drop2=[1]\n",
    "data.drop(to_drop2,inplace=True,axis=1)\n",
    "X=data.to_numpy()          #Input data space\n",
    "\n",
    "#training data set = 85% and test data set = 15%\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.15,random_state=0)\n",
    "\n",
    "#normalizing the data\n",
    "scaler=Normalizer().fit(X_train)\n",
    "X_train=scaler.transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "\n",
    "#splitting training set into validation and training\n",
    "X_train,X_val,Y_train,Y_val=train_test_split(X_train,Y_train,test_size=0.05,random_state=1)\n",
    "scaler=Normalizer().fit(X_train)\n",
    "X_train=scaler.transform(X_train)\n",
    "X_val=scaler.transform(X_val)\n",
    "\n",
    "\n",
    "#Implementing Logistic Regression\n",
    "\n",
    "#sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "#prediction function\n",
    "def predict_data(X_data,Y_data,W,bias):\n",
    "    pred_z = np.dot(X_data,W) + bias\n",
    "    pred_p = sigmoid(pred_z)\n",
    "    pred_Y =[]\n",
    "    for i in range(0,len(pred_p)):\n",
    "        if pred_p[i] >= 0.5 :\n",
    "            pred_Y.append([1])\n",
    "        else:\n",
    "            pred_Y.append([0])\n",
    "    pred_Y = np.array(pred_Y)\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for i in range(0,len(pred_Y)):\n",
    "        if pred_Y[i][0] == Y_data[i][0] and pred_Y[i] == 1:\n",
    "            TP += 1\n",
    "        elif pred_Y[i] == Y_data[i][0] and pred_Y[i] == 0:\n",
    "            TN += 1\n",
    "        elif pred_Y[i] != Y_data[i][0] and pred_Y[i] == 1:\n",
    "            FP += 1\n",
    "        elif pred_Y[i][0] != Y_data[i][0] and pred_Y[i] == 0:\n",
    "            FN += 1\n",
    "    return TP,TN,FP,FN\n",
    "\n",
    "\n",
    "losstrack1 = []\n",
    "losstrack2 = []\n",
    "accuracy_list = []\n",
    "acc_list = []\n",
    "train_list = []\n",
    "\n",
    "#Initializing weight,bias,epochs and learning rates\n",
    "W = np.random.randn(X_train.shape[1], 1)*0.99\n",
    "bias=2.99\n",
    "epochs = 1000\n",
    "learningrate = [0.5,0.7,0.9]\n",
    "\n",
    "#splitting data into batches\n",
    "batch = 50\n",
    "X_train_batch = []\n",
    "Y_train_batch = []\n",
    "for i in range(0, len(X_train), batch):\n",
    "    X_train_batch.append(X_train[i:i+batch])\n",
    "    Y_train_batch.append(Y_train[i:i+batch])\n",
    "\n",
    "for l in learningrate:\n",
    "    for epoch in range(epochs):\n",
    "        total_cost = 0\n",
    "        #for each batch we train the dataset and manipulate values for W and bias\n",
    "        for i in range(0, len(X_train_batch)):\n",
    "            m = X_train_batch[i].shape[0]\n",
    "            z = np.dot(X_train_batch[i], W) + bias\n",
    "            p = sigmoid(z)\n",
    "            dz = np.subtract(p,Y_train_batch[i])\n",
    "            \n",
    "            #Gradient descent for W and bias\n",
    "            dw = (1 / m) * np.dot(dz.transpose(),X_train_batch[i])      \n",
    "            dw=dw.transpose()\n",
    "            db = (1 / m) * np.sum(dz.transpose())\n",
    "            W = W - l * dw\n",
    "            bias = bias - l * db\n",
    "        \n",
    "        #Calculating Cross Entropy for training dataset\n",
    "        m_train = X_train.shape[0]\n",
    "        z_train = np.dot(X_train, W) + bias\n",
    "        p_train = sigmoid(z_train)\n",
    "        train_cost = -np.sum(np.multiply(np.log(p_train), Y_train) + np.multiply((1 - Y_train), np.log(1 - p_train)))/m_train\n",
    "        losstrack1.append(np.squeeze(train_cost))\n",
    "        \n",
    "        #Calculating Cross Entropy for validation dataset\n",
    "        m_val = len(X_val)\n",
    "        z_val = np.dot(X_val, W) + bias\n",
    "        p_val = sigmoid(z_val)\n",
    "        validation_cost = -np.sum(np.multiply(np.log(p_val), Y_val) + np.multiply((1 - Y_val), np.log(1 - p_val)))/m_val\n",
    "        losstrack2.append(np.squeeze(validation_cost))\n",
    "\n",
    "        #Calculating confusion matrix metrics for training dataset\n",
    "        TPtr,TNtr,FPtr,FNtr = predict_data(X_train,Y_train,W,bias)\n",
    "        accuracy_tr = (TPtr+TNtr)/(TPtr+TNtr+FPtr+FNtr)          #accuracy for training dataset\n",
    "        \n",
    "        #Calculating confusion matrix metrics for validation dataset\n",
    "        TPv,TNv,FPv,FNv = predict_data(X_val,Y_val,W,bias)\n",
    "        accuracy_v = (TPv+TNv)/(TPv+TNv+FPv+FNv)                 #accuracy for validation dataset\n",
    "        \n",
    "        accuracy_list.append(np.squeeze(accuracy_v))\n",
    "        train_list.append(np.squeeze(accuracy_tr))\n",
    "    \n",
    "    acc_list.append(np.squeeze(accuracy_v))\n",
    "\n",
    "TP,TN,FP,FN = predict_data(X_test,Y_test,W,bias)      #predicting on the test dataset\n",
    "\n",
    "print('CALCULATED VALUE OF ACCURACY, PRECISION AND RECALL ON TEST DATA FOR WDBC DATASET')\n",
    "print('--------------------------------------------------------------------------------')\n",
    "accuracy_test = (TP+TN)/(TP+TN+FP+FN)\n",
    "print('ACCURACY VALUE: ' +str(accuracy_test))\n",
    "\n",
    "precision_test = TP/(TP+FP)\n",
    "print('PRECISION VALUE: ' +str(precision_test))\n",
    "\n",
    "recall_test = TP/(TP+FN)\n",
    "print('RECALL VALUE: '+str(recall_test))\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "#Graph plotted for evaluation of the Logistic Regression model\n",
    "plt.title('Training Data and Validation Data vs Epoch')   \n",
    "plt.plot(losstrack1,color = 'blue', label ='training error',lw=1)\n",
    "plt.plot(losstrack2,color = 'red', label = 'validation error',lw=1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "\n",
    "plt.title('Accuracy of Training Data and Validation Data vs Epoch')\n",
    "plt.plot(train_list,color = 'blue', label ='training accuracy',lw=1)\n",
    "plt.plot(accuracy_list,color = 'red', label ='validation accuracy',lw=1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "\n",
    "plt.title('Accuracy of Validation Data vs Learning Rate')\n",
    "plt.plot(acc_list,color = 'red', label ='accuracy',lw=1)\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
